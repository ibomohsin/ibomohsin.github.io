---
layout: default
---

## About me

Passionate about building AI systems rooted in first principles, striving to fully understand their capabilities and limitations. I enjoy deconstructing complex problems to reveal their underlying structure, balancing rigorous technical analysis with strategic, big-picture thinking. My current research explores pushing the boundaries of parameter efficiency—through techniques such as scaling laws, shape optimization, and recursive inference—without compromising on critical dimensions like fairness, safety, and cultural diversity.


## Education

- **Ph.D.** in Computer Science at [KAUST](https://www.kaust.edu.sa/) (2012 - 2017), _GPA: 4.0 / 4.0_.
  - Thesis Title: _Learning via Query Synthesis_, Committee: X. Zhang, X. Gao, D. Keyes (KAUST), and W. Wang (UCLA).
- **M.S.** in Electrical Engineering at [Stanford University](https://www.stanford.edu/) (2009 - 2011), _GPA: 4.15 / 4.0_. 
- **B.S.** in Computer Engineering at [University of Nebraska, Lincoln](https://www.unl.edu/) (2000 - 2005),  _GPA: 3.98 / 4.0_.
  - _Highest Distinction_, _Superior Scholarship Award_, _Minor in Economics_.

## Selected Activities
- Area Chair @ [NeurIPS](https://nips.cc/) and [ICML](https://icml.cc/).
- Program committee member at [ICLR](https://iclr.cc/), [AAAI](https://aaai.org/Conferences/AAAI-22/), [CVPR](https://cvpr2023.thecvf.com/) and [ECCV](https://eccv.ecva.net/).
- [Featured](https://lnkd.in/eGPzHwi7) at the Saudi national TV on March, 2024. The episode explored my career.
- Member of the [S20 Task Force](https://s20saudiarabia.org.sa/theme.html) (Digital Revolution) that contributed to an executive report for the G20 summit, 2020.
- Chair of AI track of the 2019 [Arab-American Frontiers Symposium](https://www.nationalacademies.org/our-work/arab-american-frontiers-of-science-engineering-and-medicine), 2019.

## Books
Ibrahim Alabdulmohsin, *Summability Calculus: A Comprehensive Theory of Fractional Finite Sums*, Springer, 2018.

Many identities derived in this book were featured in websites such as Wikipedia and OEIS, including:
* **Wikipedia:** Identities relating the Euler-Mascheroni constant ($\gamma$) to approximation errors are featured in the entry for [Euler's Constant](https://en.wikipedia.org/wiki/Euler%27s_constant#Asymptotic_expansions).
* **Wikipedia:** Novel series expansions involving logarithmic numbers are featured in the entry for [Gregory Coefficients](https://en.wikipedia.org/wiki/Gregory_coefficients#Series_with_Gregory_coefficients).
* **OEIS:** Sequence data and summation results are archived in the *On-Line Encyclopedia of Integer Sequences* (e.g., [A002206](https://oeis.org/A002206)).

## Recent Preprints
- Wang, X., Alabdulmohsin, I., Salz, D., Li, Z., Rong, K. and Zhai, X., 2025. Scaling pre-training to one hundred billion data for vision language models. arXiv preprint arXiv:2502.07617.
- Tschannen, M., Gritsenko, A., Wang, X., Naeem, M.F., Alabdulmohsin, I., Parthasarathy, N., Evans, T., Beyer, L., Xia, Y., Mustafa, B. and Hénaff, O., 2025. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786.
- Steiner, A., Pinto, A.S., Tschannen, M., Keysers, D., Wang, X., Bitton, Y., Gritsenko, A., Minderer, M., Sherbondy, A., Long, S. and Qin, S., 2024. Paligemma 2: A family of versatile vlms for transfer. arXiv preprint arXiv:2412.03555.
- Beyer, L., Steiner, A., Pinto, A.S., Kolesnikov, A., Wang, X., Salz, D., Neumann, M., Alabdulmohsin, I., Tschannen, M., Bugliarello, E. and Unterthiner, T., 2024. Paligemma: A versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726.
- Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul Voigtlaender, Basil Mustafa, Sebastian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, et al:
"*PaLI-3 Vision Language Models: Smaller, Faster, Stronger.*" ArXiv: abs/2310.09199 (2023)

## Recent Publications
- Alabdulmohsin, I. and Zhai, X.: *Recursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems.* **NeurIPS**, 2025.
- Alabdulmohsin, I. and Steiner, A: *A Tale of Two Structures: Do LLMs Capture the Fractal Complexity of Language?* **ICML**, 2025.
- Ibrahim Alabdulmohsin, Vinh Q. Tran, and Mostafa Dehghani: "*Fractal Patterns May Illuminate the Success of Next-Token Prediction.*" **NeurIPS**, 2024.
- Bo Wan, Michael Tschannen, Yongqin Xian, Filip Pavetic, Ibrahim Alabdulmohsin, Xiao Wang, André Susano Pinto, Andreas Steiner, Lucas Beyer, Xiaohua Zhai: "*LocCa: Visual Pretraining with Location-aware Captioners*" **NeurIPS**, 2024.
- Angéline Pouget, Lucas Beyer, Emanuele Bugliarello, Xiao Wang, Andreas Peter Steiner, Xiaohua Zhai, Ibrahim Alabdulmohsin: "*No Filter: Cultural and Socioeconomic Diversity in Contrastive Vision-Language Models*" **NeurIPS**, 2024.
- Ibrahim Alabdulmohsin, Xiao Wang, Andreas Peter Steiner, Priya Goyal, Alexander D'Amour, Xiaohua Zhai: "*CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?*" **ICLR**, 2024.
-   Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri, Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, Bo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin Ritter, A. J. Piergiovanni, Matthias Minderer, Filip Pavetic, Austin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas Beyer, et al:
"*PaLI-X: On Scaling up a Multilingual Vision and Language Model.*" **CVPR**, 2024.
- Ibrahim Alabdulmohsin, Xiaohua Zhai, Alexander Kolesnikov, Lucas Beyer:
"*Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design.*" **NeurIPS**, 2023.
- Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, Avital Oliver, Piotr Padlewski, Alexey A. Gritsenko, Mario Lucic, Neil Houlsby: "*Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution.*"
 **NeurIPS**, 2023.
- Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, et al:
"*Scaling Vision Transformers to 22 Billion Parameters.*" **ICML**, 2023.
- Lucas Beyer, Pavel Izmailov, Alexander Kolesnikov, Mathilde Caron, Simon Kornblith, Xiaohua Zhai, Matthias Minderer, Michael Tschannen, Ibrahim Alabdulmohsin, Filip Pavetic: "*FlexiViT: One Model for All Patch Sizes*," **CVPR**, 2023.
- Ibrahim Alabdulmohsin, Nicole Chiou, Alexander D'Amour, Arthur Gretton, Sanmi Koyejo, Matt J. Kusner, Stephen R. Pfohl, Olawale Salaudeen, Jessica Schrouff, Katherine Tsai: "*Adapting to Latent Subgroup Shifts via Concepts and Proxies*," **AISTATS**, 2023.
- Ibrahim Alabdulmohsin, Behnam Neyshabur, Xiaohua Zhai: "*Revisiting Neural Scaling Laws in Language and Vision*," **NeurIPS**, 2022.
- Ibrahim Alabdulmohsin, Jessica Schrouff, Oluwasanmi Koyejo: "*A Reduction to Binary Approach for Debiasing Multiclass Datasets*,"  **NeurIPS**, 2022.
- Jessica Schrouff, Natalie Harris, Oluwasanmi Koyejo, Ibrahim Alabdulmohsin, Eva Schnider, Krista Opsahl-Ong, Alexander Brown, Subhrajit Roy, Diana Mincu, Christina Chen, Awa Dieng, Yuan Liu, Vivek Natarajan, Alan Karthikesalingam, Katherine A. Heller, Silvia Chiappa, Alexander D'Amour: "*Maintaining fairness across distribution shift: do we have viable solutions for real-world applications?*", **NeurIPS**, 2022.
- Alexander Soen, Ibrahim Alabdulmohsin, Sanmi Koyejo, Yishay Mansour, Nyalleng Moorosi, Richard Nock, Ke Sun, Lexing Xie: 
"*Fair Wrapping for Black-box Predictions*," **NeurIPS**, 2022.

